# ========== ROOT: /Users/naraptis/Desktop/Combustion/scorch ==========

# ===== annotation_loader.py =====

# scorch/annotation_loader.py
import json
from pathlib import Path
from filesystem.file_io import FileIO
from labels.image_annotation_document import ImageAnnotationDocument


class AnnotationLoadError(Exception):
    """Raised when an annotation JSON file cannot be parsed or validated."""
    pass


def load_annotation_document(path: str | Path) -> ImageAnnotationDocument:
    """
    Load an annotation JSON file safely from an absolute path.
    - We assume `path` is absolute or resolvable to absolute.
    """
    p = Path(path).resolve()

    # --- Stage 1: load raw bytes ---
    try:
        raw_bytes = FileIO.load(p)
    except Exception as e:
        raise AnnotationLoadError(f"Failed to read file: {p}") from e

    # --- Stage 2: decode UTF-8 ---
    try:
        text = raw_bytes.decode("utf-8")
    except Exception as e:
        raise AnnotationLoadError(f"Invalid UTF-8 in file: {p}") from e

    # --- Stage 3: JSON parse ---
    try:
        data = json.loads(text)
    except json.JSONDecodeError as e:
        raise AnnotationLoadError(f"Malformed JSON in file: {p}") from e

    # --- Stage 4: convert to ImageAnnotationDocument ---
    try:
        return ImageAnnotationDocument.from_json(data)
    except Exception as e:
        raise AnnotationLoadError(
            f"Invalid annotation structure in file: {p}"
        ) from e



# ===== data_loader.py =====

# scorch/data_loader.py
from __future__ import annotations
from dataclasses import dataclass
from pathlib import Path
from typing import Iterator, List

from filesystem.file_io import FileIO
from labels.image_annotation_document import ImageAnnotationDocument
from .annotation_loader import load_annotation_document, AnnotationLoadError


@dataclass
class AnnotationImagePair:
    """
    Holds a full annotation document and the matching image path on disk.
    """
    document: ImageAnnotationDocument
    image_path: Path

# scorch/data_loader.py  (continued)

class DataLoader:
    """
    Generic loader that:
      - finds all annotation JSON files via FileIO.get_all_files_local
      - loads each into an ImageAnnotationDocument
      - verifies the corresponding PNG image exists (name + ".png")
      - yields AnnotationImagePair objects
    """

    def __init__(self, annotations_subdir: str, images_subdir: str | None = None):
        """
        annotations_subdir: project-local subdirectory where *_annotations.json live
        images_subdir:      project-local subdirectory where PNG images live
                            (defaults to the same as annotations_subdir)
        """
        self.annotations_subdir = annotations_subdir
        self.images_subdir = images_subdir or annotations_subdir

        # --- Discover annotation files ---
        all_files: List[Path] = FileIO.get_all_files_local(
            subdirectory=self.annotations_subdir
        )

        # Keep only files that look like annotation JSONs.
        self.annotation_files: List[Path] = sorted(
            f for f in all_files
            if f.suffix == ".json" and f.name.endswith("_annotations.json")
        )
        
        seen = set()
        self.documents = []
        class_names = set()
        for annotation_path in self.annotation_files:
            try:
                document = load_annotation_document(annotation_path)
                name = document.name
                if name in seen:
                    print("DUPE NAME??? ", name)
                    continue
                seen.add(name)
                self.documents.append(document)
                for label_name in document.data_label_names:
                    class_names.add(label_name)
            except AnnotationLoadError as e:
                print(f"[DataLoader] Skipping corrupt file: {annotation_path}\n  {e}")
                continue  # skip but keep going
        self.class_names = sorted(list(class_names))

    def __len__(self) -> int:
        return len(self.annotation_files)

    def __iter__(self):
        for document in self.documents:
            name = document.name

            # build image path
            img_rel = Path(self.images_subdir) / f"{name}.png"
            img_path = FileIO.local_file(name=str(img_rel)).resolve()

            if not img_path.exists():
                print(f"[DataLoader] Missing PNG for '{name}', skipping.")
                continue

            yield AnnotationImagePair(document=document, image_path=img_path)




# ===== grad_check.py =====

# scorch/grad_check.py
from __future__ import annotations

import numpy as np

from scorch.scorch_sequential import ScorchSequential
from scorch.scorch_linear_2 import ScorchLinear2
from scorch.scorch_relu_2 import ScorchReLU2


def softmax_and_cross_entropy_with_grad(logits: np.ndarray, target_index: int):
    """
    Same as in your runner, but local here for convenience.
    """
    shifted = logits - np.max(logits)
    exp = np.exp(shifted)
    probs = exp / np.sum(exp)

    loss = -np.log(probs[target_index] + 1e-12)

    grad = probs.copy()
    grad[target_index] -= 1.0

    return float(loss), grad


def compute_loss(model: ScorchSequential, x: np.ndarray, target_index: int) -> float:
    """
    Forward through the model and return scalar loss for a single (x, y).
    No gradient computation here, pure forward.
    """
    logits = model.forward(x)  # (C,)
    loss, _ = softmax_and_cross_entropy_with_grad(logits, target_index)
    return loss


def compute_loss_and_backprop(model: ScorchSequential, x: np.ndarray, target_index: int):
    """
    Forward + backward for a single sample.
    Returns:
        loss (float)
    and fills model's gradients via backward().
    """
    model.zero_grad()

    logits = model.forward(x)
    loss, grad_logits = softmax_and_cross_entropy_with_grad(logits, target_index)
    _ = model.backward(grad_logits)

    return loss


def grad_check_weight(
    model: ScorchSequential,
    x: np.ndarray,
    target_index: int,
    layer_idx: int,
    i: int,
    j: int,
    eps: float = 1e-4,
):
    """
    Compare analytic grad_W[i,j] vs numeric finite-difference gradient on a given layer.

    layer_idx:
        index in model.layers where the ScorchLinear2 lives.

    i, j:
        indices into that layer's W matrix.
    """
    layer = model.layers[layer_idx]
    if not isinstance(layer, ScorchLinear2):
        raise TypeError(f"Layer at index {layer_idx} is not ScorchLinear2: {type(layer)}")

    W = layer.W

    # --- 1) Analytic gradient via backprop ---
    _ = compute_loss_and_backprop(model, x, target_index)
    analytic = layer.grad_W[i, j]

    # --- 2) Numeric gradient via finite differences ---
    original = W[i, j]

    # W[i, j] + eps
    W[i, j] = original + eps
    loss_plus = compute_loss(model, x, target_index)

    # W[i, j] - eps
    W[i, j] = original - eps
    loss_minus = compute_loss(model, x, target_index)

    # Restore original weight
    W[i, j] = original

    numeric = (loss_plus - loss_minus) / (2.0 * eps)

    # --- 3) Print comparison ---
    print(f"[grad_check] layer={layer_idx}, W[{i},{j}]")
    print(f"  analytic: {analytic}")
    print(f"  numeric : {numeric}")
    print(f"  diff    : {abs(analytic - numeric)}")

    return analytic, numeric



# ===== nn_functional.py =====

# scorch/nn_functional.py
from __future__ import annotations
import numpy as np

def linear_forward(x: np.ndarray, W: np.ndarray, b: np.ndarray) -> np.ndarray:
    """

    W is the matrix of learned weights.
    linear_forward applies those weights to
    the input features to compute the output
    neuron activations. These activations
    propagate forward to the next layer,
    or become the final “logits” used
    by the network to make predictions.

    Pure function: y = W @ x + b

    x: (D,)
    W: (C, D)
    b: (C,)
    returns: (C,)
    """

    #return W @ x + b

    
    C = W.shape[0]   # number of output neurons
    D = W.shape[1]   # number of input features

    # Safety check
    if x.shape[0] != D:
        raise ValueError(f"linear_forward: Expected x of shape ({D},), got {x.shape}")

    z = [0.0 for _ in range(C)]

    for i in range(C):         # for every output neuron
        acc = 0.0
        for j in range(D):     # multiply across the row of W
            acc += W[i, j] * x[j]
        acc += b[i]            # add bias
        z[i] = acc

    return np.array(z, dtype=np.float32)


def softmax_cross_entropy(logits: np.ndarray, target_index: int) -> float:
    """
    Pure function: softmax + cross-entropy loss for a single example.

    logits is the output from linear forward … a 1-D list of floats.
    target_index is the correct class label as an integer.

    logits: (C,)
    target_index: int in [0, C)

    Classes:
        0 = cat
        1 = dog
        2 = fish

    logits = [1.5, 0.2, -0.3]
    target_index = 2 (this is expected to be "fish")

    Interpretation:
        cat: 1.5
        dog: 0.2
        fish: -0.3

    ...

    exp([1.5, 0.2, -0.3]) → normalized → probs

    probs ≈ [0.65, 0.23, 0.12]

    Which means:
        Model thinks “cat” with probability 65%
        Model thinks “dog” with probability 23%
        Model thinks “fish” with probability 12%

    This was expected as fish, so it will be a high loss...

    loss = - log(prob_of_correct_class)
    loss = - log(0.12)
    loss ≈ 2.12  (a high loss)

    """
    # For numerical stability
    shifted = logits - np.max(logits)
    exp = np.exp(shifted)
    probs = exp / np.sum(exp)

    # Cross entropy loss: -log p_target
    loss = -np.log(probs[target_index] + 1e-12)
    return float(loss)



# ===== scorch_conv2d.py =====

# scorch/scorch_conv2d.py
from __future__ import annotations

import numpy as np
from scorch.scorch_module import ScorchModule


class ScorchConv2d(ScorchModule):
    """
    Pure loop-based reference implementation of a 2D convolution.

    Follows PyTorch semantics exactly:

        input:   (N, C_in,  H_in,  W_in)
        weight:  (C_out, C_in, K_h, K_w)
        bias:    (C_out,)
        output:  (N, C_out, H_out, W_out)

    H_out, W_out computed using the standard formula:

        H_out = floor((H_in + 2*pad_h - dilation_h*(K_h - 1) - 1) / stride_h + 1)
        W_out = floor((W_in + 2*pad_w - dilation_w*(K_w - 1) - 1) / stride_w + 1)

    All operations implemented via explicit nested python loops.
    Ideal for debugging and correctness verification.
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int | tuple[int, int],
        stride: int | tuple[int, int] = 1,
        padding: int | tuple[int, int] = 0,
        dilation: int | tuple[int, int] = 1,
        name: str | None = None,
    ):
        super().__init__()

        self.in_channels = int(in_channels)
        self.out_channels = int(out_channels)

        # Normalize kernel size
        if isinstance(kernel_size, int):
            self.k_h = self.k_w = kernel_size
        else:
            self.k_h, self.k_w = kernel_size

        # Normalize stride
        if isinstance(stride, int):
            self.s_h = self.s_w = stride
        else:
            self.s_h, self.s_w = stride

        # Normalize padding
        if isinstance(padding, int):
            self.p_h = self.p_w = padding
        else:
            self.p_h, self.p_w = padding

        # Normalize dilation
        if isinstance(dilation, int):
            self.d_h = self.d_w = dilation
        else:
            self.d_h, self.d_w = dilation

        self.name = name or (
            f"ScorchConv2d(in={in_channels}, out={out_channels}, "
            f"kernel=({self.k_h},{self.k_w}), stride=({self.s_h},{self.s_w}), "
            f"padding=({self.p_h},{self.p_w}), dilation=({self.d_h},{self.d_w}))"
        )

        # ----------------------------------------------------------
        # Parameter initialization (Kaiming-ish)
        # W shape: (C_out, C_in, K_h, K_w)
        # b shape: (C_out,)
        # ----------------------------------------------------------
        fan_in = self.in_channels * self.k_h * self.k_w
        limit = 1.0 / np.sqrt(fan_in)

        self.W = np.random.uniform(
            -limit, +limit,
            size=(self.out_channels, self.in_channels, self.k_h, self.k_w)
        ).astype(np.float32)

        self.b = np.zeros((self.out_channels,), dtype=np.float32)

        # Gradient buffers
        self.grad_W = np.zeros_like(self.W)
        self.grad_b = np.zeros_like(self.b)

        # Cache for backward
        self._last_x = None
        self._last_output_shape = None

    # --------------------------------------------------------------
    # Forward
    # --------------------------------------------------------------
    def forward(self, x):
        """
        x: np.ndarray (N, C_in, H_in, W_in)
        returns: np.ndarray (N, C_out, H_out, W_out)
        """
        x = np.asarray(x, dtype=np.float32)

        if x.ndim != 4:
            raise ValueError(
                f"{self.name}: Expected 4D input (N,C,H,W), got {x.shape}"
            )

        N, C_in, H_in, W_in = x.shape

        if C_in != self.in_channels:
            raise ValueError(
                f"{self.name}: Expected {self.in_channels} input channels, "
                f"got {C_in}"
            )

        # Compute output shape
        H_out = ((H_in + 2*self.p_h - self.d_h*(self.k_h - 1) - 1)
                 // self.s_h + 1)

        W_out = ((W_in + 2*self.p_w - self.d_w*(self.k_w - 1) - 1)
                 // self.s_w + 1)

        # Allocate output
        y = np.zeros((N, self.out_channels, H_out, W_out), dtype=np.float32)

        # Pad the input
        x_padded = np.pad(
            x,
            pad_width=(
                (0, 0),     # batch
                (0, 0),     # channels
                (self.p_h, self.p_h),
                (self.p_w, self.p_w),
            ),
            mode="constant",
            constant_values=0.0,
        )

        # ----------------------------------------------------------
        # Pure nested loops:
        #   for each batch, each output channel,
        #   each output pixel, each input channel,
        #   each kernel element...
        # ----------------------------------------------------------
        for n in range(N):
            for c_out in range(self.out_channels):
                for oy in range(H_out):
                    in_y_origin = oy * self.s_h
                    for ox in range(W_out):
                        in_x_origin = ox * self.s_w

                        acc = self.b[c_out]

                        for c_in in range(self.in_channels):
                            for ky in range(self.k_h):
                                iy = in_y_origin + ky * self.d_h
                                for kx in range(self.k_w):
                                    ix = in_x_origin + kx * self.d_w

                                    acc += (
                                        self.W[c_out, c_in, ky, kx] *
                                        x_padded[n, c_in, iy, ix]
                                    )

                        y[n, c_out, oy, ox] = acc

        # Save for backward
        self._last_x = x
        self._last_output_shape = (H_out, W_out)

        return y

    # --------------------------------------------------------------
    # Backward
    # --------------------------------------------------------------
    def backward(self, grad_out):
        """
        grad_out: np.ndarray (N, C_out, H_out, W_out)
        returns: grad_input (N, C_in, H_in, W_in)
        """
        if self._last_x is None:
            raise RuntimeError(f"{self.name}: backward called before forward.")

        x = self._last_x
        N, C_in, H_in, W_in = x.shape
        H_out, W_out = self._last_output_shape

        grad_out = np.asarray(grad_out, dtype=np.float32)

        # Allocate gradients
        grad_x = np.zeros_like(x, dtype=np.float32)
        self.grad_W.fill(0.0)
        self.grad_b.fill(0.0)

        # Pad x and grad_x so indexing aligns
        x_pad = np.pad(
            x,
            ((0,0),(0,0),(self.p_h,self.p_h),(self.p_w,self.p_w)),
            mode="constant",
        )
        grad_x_pad = np.zeros_like(x_pad, dtype=np.float32)

        # --------------------------------------------------------------
        # Compute gradients:
        #   grad_b[c_out] += grad_out[:,c_out,:,:]
        #   grad_W += x * grad_out
        #   grad_x += W * grad_out
        # --------------------------------------------------------------
        for n in range(N):
            for c_out in range(self.out_channels):
                for oy in range(H_out):
                    in_y_origin = oy * self.s_h
                    for ox in range(W_out):
                        in_x_origin = ox * self.s_w

                        go = grad_out[n, c_out, oy, ox]

                        # Bias gradient
                        self.grad_b[c_out] += go

                        # Weight + Input gradients
                        for c_in in range(self.in_channels):
                            for ky in range(self.k_h):
                                iy = in_y_origin + ky * self.d_h
                                for kx in range(self.k_w):
                                    ix = in_x_origin + kx * self.d_w

                                    # dL/dW
                                    self.grad_W[c_out, c_in, ky, kx] += (
                                        x_pad[n, c_in, iy, ix] * go
                                    )

                                    # dL/dx
                                    grad_x_pad[n, c_in, iy, ix] += (
                                        self.W[c_out, c_in, ky, kx] * go
                                    )

        # Remove padding from grad_x
        grad_x = grad_x_pad[
            :,
            :,
            self.p_h:self.p_h+H_in,
            self.p_w:self.p_w+W_in,
        ]

        return grad_x

    # --------------------------------------------------------------
    # Parameter handling
    # --------------------------------------------------------------
    def parameters(self):
        return [self.W, self.b]

    def zero_grad(self):
        self.grad_W.fill(0.0)
        self.grad_b.fill(0.0)

    def __repr__(self):
        return self.name



# ===== scorch_conv2d_fast.py =====

# scorch/scorch_conv2d_fast.py
from __future__ import annotations

import numpy as np
from scorch.scorch_module import ScorchModule


def im2col(
    x, 
    k_h, k_w, 
    s_h, s_w, 
    p_h, p_w, 
    d_h, d_w
):
    """
    Convert (N, C, H, W) → (N, C*k_h*k_w, H_out*W_out) using im2col.
    """

    N, C, H, W = x.shape

    H_out = ((H + 2*p_h - d_h*(k_h - 1) - 1) // s_h) + 1
    W_out = ((W + 2*p_w - d_w*(k_w - 1) - 1) // s_w) + 1

    # Pad input
    x_padded = np.pad(
        x,
        pad_width=(
            (0, 0),
            (0, 0),
            (p_h, p_h),
            (p_w, p_w)
        ),
        mode="constant"
    )

    # Allocate output matrix
    cols = np.zeros((N, C * k_h * k_w, H_out * W_out), dtype=np.float32)

    out_col = 0
    for oy in range(H_out):
        in_y_origin = oy * s_h
        for ox in range(W_out):
            in_x_origin = ox * s_w

            # Gather all kernel positions for this output pixel
            patch_idx = 0
            for c in range(C):
                for ky in range(k_h):
                    iy = in_y_origin + ky * d_h
                    for kx in range(k_w):
                        ix = in_x_origin + kx * d_w

                        cols[:, patch_idx, out_col] = x_padded[:, c, iy, ix]
                        patch_idx += 1

            out_col += 1

    return cols  # (N, C*k_h*k_w, H_out*W_out)


def col2im(
    cols, 
    x_shape, 
    k_h, k_w, 
    s_h, s_w, 
    p_h, p_w, 
    d_h, d_w
):
    """
    Inverse of im2col:
    Convert (N, C*k_h*k_w, H_out*W_out) → (N, C, H, W)
    by adding contributions back into padded input space.
    """

    N, C, H, W = x_shape

    H_out = ((H + 2*p_h - d_h*(k_h - 1) - 1) // s_h) + 1
    W_out = ((W + 2*p_w - d_w*(k_w - 1) - 1) // s_w) + 1

    x_padded = np.zeros((N, C, H + 2*p_h, W + 2*p_w), dtype=np.float32)

    out_col = 0
    for oy in range(H_out):
        in_y_origin = oy * s_h
        for ox in range(W_out):
            in_x_origin = ox * s_w

            patch_idx = 0
            for c in range(C):
                for ky in range(k_h):
                    iy = in_y_origin + ky * d_h
                    for kx in range(k_w):
                        ix = in_x_origin + kx * d_w

                        x_padded[:, c, iy, ix] += cols[:, patch_idx, out_col]
                        patch_idx += 1

            out_col += 1

    # Remove padding
    return x_padded[:, :, p_h:p_h+H, p_w:p_w+W]


class ScorchConv2dFast(ScorchModule):
    """
    High-performance im2col + matmul Conv2d.
    Produces identical results to ScorchConv2d (nested loop version),
    but runs dramatically faster.
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int | tuple[int, int],
        stride: int | tuple[int, int] = 1,
        padding: int | tuple[int, int] = 0,
        dilation: int | tuple[int, int] = 1,
        name: str | None = None,
    ):
        super().__init__()

        self.in_channels = int(in_channels)
        self.out_channels = int(out_channels)

        # Normalize kernel size
        if isinstance(kernel_size, int):
            self.k_h = self.k_w = kernel_size
        else:
            self.k_h, self.k_w = kernel_size

        # Normalize stride
        if isinstance(stride, int):
            self.s_h = self.s_w = stride
        else:
            self.s_h, self.s_w = stride

        # Normalize padding
        if isinstance(padding, int):
            self.p_h = self.p_w = padding
        else:
            self.p_h, self.p_w = padding

        # Normalize dilation
        if isinstance(dilation, int):
            self.d_h = self.d_w = dilation
        else:
            self.d_h, self.d_w = dilation

        self.name = name or (
            f"ScorchConv2dFast(in={in_channels}, out={out_channels}, "
            f"kernel=({self.k_h},{self.k_w}), stride=({self.s_h},{self.s_w}), "
            f"padding=({self.p_h},{self.p_w}), dilation=({self.d_h},{self.d_w}))"
        )

        # Parameter initialization
        fan_in = self.in_channels * self.k_h * self.k_w
        limit = 1.0 / np.sqrt(fan_in)

        self.W = np.random.uniform(
            -limit, +limit,
            size=(self.out_channels, self.in_channels, self.k_h, self.k_w)
        ).astype(np.float32)

        self.b = np.zeros((self.out_channels,), dtype=np.float32)

        # Grad buffers
        self.grad_W = np.zeros_like(self.W)
        self.grad_b = np.zeros_like(self.b)

        # Cache
        self._last_cols = None
        self._last_x_shape = None
        self._H_out = None
        self._W_out = None

    # ----------------------------------------------------------
    # Forward
    # ----------------------------------------------------------
    def forward(self, x):
        x = np.asarray(x, dtype=np.float32)

        if x.ndim != 4:
            raise ValueError(
                f"{self.name}: Expected 4D input (N,C,H,W), got {x.shape}"
            )

        N, C, H, W = x.shape

        # Compute output dims
        H_out = ((H + 2*self.p_h - self.d_h*(self.k_h - 1) - 1) // self.s_h) + 1
        W_out = ((W + 2*self.p_w - self.d_w*(self.k_w - 1) - 1) // self.s_w) + 1

        # Store shapes for backward
        self._last_x_shape = (N, C, H, W)
        self._H_out = H_out
        self._W_out = W_out

        # Convert input into column matrix
        cols = im2col(
            x,
            self.k_h, self.k_w,
            self.s_h, self.s_w,
            self.p_h, self.p_w,
            self.d_h, self.d_w
        )  # (N, C*k_h*k_w, H_out*W_out)

        self._last_cols = cols

        # Reshape weights for matmul
        W_mat = self.W.reshape(self.out_channels, -1)  # (C_out, C_in*k_h*k_w)

        # Allocate output
        y = np.zeros((N, self.out_channels, H_out, W_out), dtype=np.float32)

        # For each batch:
        for n in range(N):
            # matmul: (C_out, K) @ (K, H_out*W_out)
            y_n = W_mat @ cols[n] + self.b.reshape(-1, 1)
            y[n] = y_n.reshape(self.out_channels, H_out, W_out)

        return y

    # ----------------------------------------------------------
    # Backward
    # ----------------------------------------------------------
    def backward(self, grad_out):
        """
        grad_out: (N, C_out, H_out, W_out)
        returns: grad_x of shape (N, C_in, H, W)
        """
        if self._last_cols is None:
            raise RuntimeError(f"{self.name}: backward called before forward.")

        cols = self._last_cols
        N, C, H, W = self._last_x_shape
        H_out = self._H_out
        W_out = self._W_out

        grad_out = np.asarray(grad_out, dtype=np.float32)

        # Flatten grad_out for matmul
        grad_out_2d = grad_out.reshape(N, self.out_channels, H_out * W_out)

        # Reset grads
        self.grad_W.fill(0.0)
        self.grad_b.fill(0.0)

        # For matmul shapes
        W_mat = self.W.reshape(self.out_channels, -1)      # (C_out, K)
        grad_W_mat = np.zeros_like(W_mat)                  # (C_out, K)

        # Output gradients accumulate into params
        for n in range(N):
            go = grad_out_2d[n]   # (C_out, H_out*W_out)

            # grad_b
            self.grad_b += go.sum(axis=1)

            # grad_W_mat += go @ cols[n].T
            grad_W_mat += go @ cols[n].transpose(1, 0)

        # Reshape grad_W back
        self.grad_W = grad_W_mat.reshape(
            self.out_channels, self.in_channels, self.k_h, self.k_w
        )

        # --------------------------------------------------
        # Compute grad_x via W^T * grad_out, then col2im
        # --------------------------------------------------
        grad_cols = np.zeros_like(cols)

        W_mat_T = W_mat.T  # (K, C_out)

        for n in range(N):
            go = grad_out_2d[n]       # (C_out, H_out*W_out)
            grad_cols[n] = W_mat_T @ go   # (K, H_out*W_out)

        # Convert column gradients back to image shape
        grad_x = col2im(
            grad_cols,
            (N, C, H, W),
            self.k_h, self.k_w,
            self.s_h, self.s_w,
            self.p_h, self.p_w,
            self.d_h, self.d_w
        )

        return grad_x

    # ----------------------------------------------------------
    # Parameter handling
    # ----------------------------------------------------------
    def parameters(self):
        return [self.W, self.b]

    def zero_grad(self):
        self.grad_W.fill(0.0)
        self.grad_b.fill(0.0)

    def __repr__(self):
        return self.name



# ===== scorch_dataset.py =====

# scorch/scorch_dataset.py

from __future__ import annotations
from typing import List, Tuple, Dict

import torch
from torch.utils.data import Dataset

from scorch.data_loader import DataLoader
from scorch.tensor_load_helpers import iter_label_patches_from_pair
from scorch.scorch_tensor import ScorchTensor


class ScorchPatchClassificationDataset(Dataset):
    """
    Simple classification dataset:
      - walks your DataLoader (annotations + images)
      - uses image patches (from PixelBag frames) as inputs
      - uses label.name as the class
      - ignores masks for now (we're just doing classification)
    """

    def __init__(
        self,
        annotations_subdir: str,
        images_subdir: str | None = None,
        grayscale: bool = True,
    ) -> None:
        super().__init__()

        self.grayscale = grayscale
        self.class_to_index: Dict[str, int] = {}
        self.index_to_class: List[str] = []
        self.samples: List[Tuple[ScorchTensor, int]] = []

        loader = DataLoader(
            annotations_subdir=annotations_subdir,
            images_subdir=images_subdir,
        )

        for pair in loader:
            for img_tensor, _mask_tensor, label_name in iter_label_patches_from_pair(
                pair, grayscale=self.grayscale
            ):
                class_idx = self._get_or_add_class_index(label_name)
                self.samples.append((img_tensor, class_idx))

        print(f"[ScorchDataset] Loaded {len(self.samples)} samples "
              f"from {len(self.class_to_index)} classes.")

    # ----------------------------------------
    # Internal helpers
    # ----------------------------------------
    def _get_or_add_class_index(self, label_name: str) -> int:
        if label_name not in self.class_to_index:
            idx = len(self.index_to_class)
            self.class_to_index[label_name] = idx
            self.index_to_class.append(label_name)
        return self.class_to_index[label_name]

    # ----------------------------------------
    # Dataset interface
    # ----------------------------------------
    def __len__(self) -> int:
        return len(self.samples)

    def __getitem__(self, idx: int):
        scorch_tensor, class_idx = self.samples[idx]

        # Convert to PyTorch tensor (float32, C,H,W)
        x = scorch_tensor.to_torch()           # torch.float32, C,H,W
        y = torch.tensor(class_idx, dtype=torch.long)
        return x, y



# ===== scorch_linear.py =====

# scorch/scorch_linear.py
from __future__ import annotations
import math
import numpy as np

from scorch.nn_functional import linear_forward
from scorch.scorch_module import ScorchModule


class ScorchLinear(ScorchModule):
    """
    A minimal fully-connected layer:
        y = W @ x + b

    Shapes (conceptually):
        Input features:  D = in_features
        Output features: C = out_features

    Forward:
        x: (..., D)
        W: (C, D)
        b: (C,)
        y: (..., C)

        That is:
            - if x is 1-D, shape (D,), y is (C,)
            - if x is 2-D, shape (N, D), y is (N, C)
            - if x is 3-D, shape (B, T, D), y is (B, T, C)
            - etc.

    Backward:
        grad_output: dL/dy, same shape as y = (..., C)
        Produces:
            grad_W: dL/dW, shape (C, D)
            grad_b: dL/db, shape (C,)
            grad_input: dL/dx, shape (..., D)
    """

    def __init__(self, in_features: int, out_features: int, name: str | None = None):
        super().__init__()
        self.in_features = int(in_features)
        self.out_features = int(out_features)
        self.name = name or f"ScorchLinear({in_features}->{out_features})"

        # Parameter initialization (Xavier-ish)
        limit = 1.0 / math.sqrt(self.in_features)
        self.W = np.random.uniform(
            -limit, +limit,
            size=(self.out_features, self.in_features)
        ).astype(np.float32)

        self.b = np.zeros((self.out_features,), dtype=np.float32)

        # Gradients
        self.grad_W = np.zeros_like(self.W)
        self.grad_b = np.zeros_like(self.b)

        # Cache for backward: we need x with its full shape
        self._last_x: np.ndarray | None = None

    # ------------------------------------------------------
    # Forward pass
    # ------------------------------------------------------
    def forward(self, x):
        """
        x: np.ndarray of shape (..., in_features)
        returns: np.ndarray of shape (..., out_features)
        """
        x_arr = np.asarray(x, dtype=np.float32)

        if x_arr.ndim < 1:
            raise ValueError(
                f"{self.name}: Expected at least 1-D input (..., D), got {x_arr.shape}"
            )

        if x_arr.shape[-1] != self.in_features:
            raise ValueError(
                f"{self.name}: Expected last dimension {self.in_features}, "
                f"got {x_arr.shape[-1]}"
            )

        # Save for backward (full shape, not flattened)
        self._last_x = x_arr.copy()

        # ------------------------------------------------------------------
        # Conceptual NumPy one-liner (kept for reference)
        #
        #   out = x_arr @ self.W.T + self.b
        #
        # However, we expand this into manual Python loops so each multiply /
        # add is explicit for learning and debugging.
        # ------------------------------------------------------------------

        # Handle the simple 1-D case (D,) directly via linear_forward:
        if x_arr.ndim == 1:
            # (Old behavior, still here and still readable)
            # return self.W @ x_arr + self.b
            return linear_forward(x_arr, self.W, self.b)

        # General N-D case: (..., D)
        original_shape = x_arr.shape          # (..., D)
        leading_shape = original_shape[:-1]   # ...
        D = self.in_features
        C = self.out_features

        # Flatten all leading dimensions into a single "batch" dimension.
        batch_size = int(np.prod(leading_shape)) if leading_shape else 1

        x_flat = x_arr.reshape(batch_size, D)            # (B, D)
        out_flat = np.zeros((batch_size, C), dtype=np.float32)  # (B, C)

        # Manual forward: for each sample n, and each output neuron i,
        # compute dot(W[i, :], x[n, :]) + b[i]
        for n in range(batch_size):          # each sample in the batch
            for i in range(C):              # each output neuron
                acc = 0.0
                for j in range(D):          # each input feature
                    acc += self.W[i, j] * x_flat[n, j]
                acc += self.b[i]
                out_flat[n, i] = acc

        # Reshape back to (..., C)
        out = out_flat.reshape(*leading_shape, C)
        return out

    # ------------------------------------------------------
    # Backward pass
    # ------------------------------------------------------
    def backward(self, grad_output):
        """
        grad_output: dL/dy, same shape as forward output (..., out_features)

        Computes:
            grad_W (accumulated into self.grad_W)   shape (C, D)
            grad_b (accumulated into self.grad_b)   shape (C,)
        Returns:
            grad_input: dL/dx, shape (..., in_features)
        """
        if self._last_x is None:
            raise RuntimeError(f"{self.name}: backward called before forward.")

        g_out = np.asarray(grad_output, dtype=np.float32)
        x = self._last_x

        if g_out.ndim < 1:
            raise ValueError(
                f"{self.name}: grad_output must be at least 1-D (..., C), "
                f"got {g_out.shape}"
            )

        if g_out.shape[-1] != self.out_features:
            raise ValueError(
                f"{self.name}: grad_output last dimension {g_out.shape[-1]} "
                f"does not match (out_features,) = ({self.out_features},)"
            )

        if x.shape[:-1] != g_out.shape[:-1]:
            raise ValueError(
                f"{self.name}: grad_output leading shape {g_out.shape[:-1]} "
                f"does not match input leading shape {x.shape[:-1]}"
            )

        D = self.in_features
        C = self.out_features

        # ------------------------------------------------------------------
        # 1-D special case: keep it as close as possible to your old code
        # ------------------------------------------------------------------
        if x.ndim == 1 and g_out.ndim == 1:
            # x: (D,)
            # g_out: (C,)

            # dL/dW = outer(grad_output, x)
            # grad_W = np.outer(g_out, x)  # (C, D)

            # grad_W is the rate of change in error loss with respect to weight.
            # ...
            # The derivative of the loss with respect to weight W[i,j].
            # ...
            # grad_W[i,j] ==> “How much would the total loss change if we
            # nudged weight W[i,j] upward by a tiny amount?”

            grad_W = np.zeros((C, D), dtype=np.float32)
            for i in range(C):        # for each output neuron
                for j in range(D):    # for each input feature
                    grad_W[i][j] = g_out[i] * x[j]

            # dL/db = grad_output
            grad_b = g_out  # (C,)

            # dL/dx = W^T @ grad_output
            # ...
            # grad_input = Wᵀ @ g_out because each input
            # feature receives error signals from every output
            # neuron, scaled by the weight that connects them...
            # and that weighted sum is a dot product.

            # grad_input = self.W.T @ g_out  # (D,)

            grad_input = np.zeros(D, dtype=np.float32)
            for j in range(D):               # for each input feature x[j]
                acc = 0.0
                for i in range(C):           # sum over all output neurons
                    acc += self.W[i][j] * g_out[i]
                grad_input[j] = acc

            # Accumulate gradients (so multiple samples can add up)
            self.grad_W += grad_W
            self.grad_b += grad_b

            return grad_input

        # ------------------------------------------------------------------
        # General N-D case: x shape (..., D), grad_output shape (..., C)
        #
        # We flatten to:
        #   x_flat:       (B, D)
        #   g_out_flat:   (B, C)
        #
        # Then:
        #   grad_W[i,j] = Σ_n  g_out[n,i] * x[n,j]
        #   grad_b[i]   = Σ_n  g_out[n,i]
        #   grad_input[n,j] = Σ_i W[i,j] * g_out[n,i]
        # ------------------------------------------------------------------
        leading_shape = x.shape[:-1]
        batch_size = int(np.prod(leading_shape)) if leading_shape else 1

        x_flat = x.reshape(batch_size, D)            # (B, D)
        g_out_flat = g_out.reshape(batch_size, C)    # (B, C)

        grad_W = np.zeros((C, D), dtype=np.float32)
        grad_b = np.zeros((C,), dtype=np.float32)
        grad_input_flat = np.zeros((batch_size, D), dtype=np.float32)

        # Manual accumulation over the batch and feature dimensions
        for n in range(batch_size):       # each sample
            for i in range(C):           # each output neuron
                gi = g_out_flat[n, i]    # gradient flowing into neuron i
                grad_b[i] += gi          # dL/db accumulates over batch

                # grad_W[i,j] = Σ_n g[n,i] * x[n,j]
                # grad_input[n,j] = Σ_i W[i,j] * g[n,i]
                for j in range(D):
                    xnj = x_flat[n, j]
                    grad_W[i, j] += gi * xnj
                    grad_input_flat[n, j] += self.W[i, j] * gi

        # Reshape grad_input back to (..., D)
        grad_input = grad_input_flat.reshape(*leading_shape, D)

        # Accumulate gradients (so multiple samples / batches can add up)
        self.grad_W += grad_W
        self.grad_b += grad_b

        return grad_input

    # ------------------------------------------------------
    # Parameter + gradient handling
    # ------------------------------------------------------
    def parameters(self):
        return [self.W, self.b]

    def zero_grad(self):
        self.grad_W.fill(0.0)
        self.grad_b.fill(0.0)

    def __repr__(self):
        return f"{self.name}(W={self.W.shape}, b={self.b.shape})"



# ===== scorch_max_pool2d.py =====

# scorch_max_pool2d.py

#dead ChatGPT, let's make a perfect clone of this for our honorific framework.


# ===== scorch_module.py =====

# scorch/scorch_module.py
from __future__ import annotations

from typing import List, Any


class ScorchModule:
    """
    Minimal base class for all Scorch layers.

    Design goals:
      - Very small surface area.
      - Forward and backward are explicit.
      - Parameter handling is unified (so optimizers can just call .parameters()).

    Subclasses should override:
      - forward(self, x)
      - backward(self, grad_output)
      - parameters(self)         (if they have learnable params)
      - zero_grad(self)          (if they store gradients)
    """

    def forward(self, x: Any) -> Any:
        """
        Compute the forward pass.

        Must be overridden in subclasses.

        Example signature in subclasses:
            x: np.ndarray
            returns: np.ndarray

        Forward = compute values
        Backward = compute sensitivities
        """
        raise NotImplementedError(f"{self.__class__.__name__}.forward not implemented.")

    def backward(self, grad_output: Any) -> Any:
        """
        Compute the backward pass.

        grad_output is dL/d(out) from the next layer.
        This method must compute dL/d(input) and store
        gradients for parameters (if any).

        Chain Rule:
        (dL / dx) = (dL / dy) * (dy / dx)

        (The gradient leaving a node) =
        (The gradient entering that node) *
        (the derivative of the operation the node performs)

        Forward = compute values
        Backward = compute sensitivities
        """
        raise NotImplementedError(f"{self.__class__.__name__}.backward not implemented.")

    # --------------------------------------------------
    # Parameter handling
    # --------------------------------------------------
    def parameters(self) -> List[Any]:
        """
        Return a flat list of all learnable parameters
        owned by this module.

        Layers without parameters can just inherit this
        default (empty list).
        """
        return []

    def zero_grad(self) -> None:
        """
        Reset gradients for all learnable parameters.

        Layers without parameters (e.g. ReLU, MaxPool)
        can inherit this default no-op implementation.
        Layers with parameters should override and zero
        their internal grad arrays.
        """
        # No-op by default
        pass

    # --------------------------------------------------
    # Convenience: allow calling module(x) like in PyTorch
    # --------------------------------------------------
    def __call__(self, x: Any) -> Any:
        return self.forward(x)

    def __repr__(self) -> str:
        return f"{self.__class__.__name__}()"



# ===== scorch_relu.py =====

# scorch/scorch_relu.py
from __future__ import annotations

import numpy as np
from scorch.scorch_module import ScorchModule


class ScorchReLU(ScorchModule):
    """
    Elementwise ReLU activation:

        y = max(0, x)

    Forward:
        - Works on any np.ndarray shape.
        - Stores a mask (x > 0) for backward.

    Backward:
        - grad_input = grad_output * (x > 0)
    """

    def __init__(self, name: str | None = None):
        super().__init__()
        self.name = name or "ScorchReLU"
        self._mask: np.ndarray | None = None

    # ------------------------------------------------------
    # Forward pass
    # ------------------------------------------------------
    def forward(self, x):
        """
        x: np.ndarray (any shape)
        returns: np.ndarray (same shape)
        """
        x_arr = np.asarray(x, dtype=np.float32)

        # Store mask for backward: 1 where x > 0, 0 elsewhere
        self._mask = (x_arr > 0).astype(np.float32)

        # ReLU: max(0, x) == x * (x > 0)
        return x_arr * self._mask

    # ------------------------------------------------------
    # Backward pass
    # ------------------------------------------------------
    def backward(self, grad_output):
        """
        grad_output: dL/dy, same shape as forward output
        returns: dL/dx, same shape as input

        For ReLU:
            dL/dx = dL/dy * 1 where x > 0
            dL/dx = dL/dy * 0 where x <= 0
        """
        if self._mask is None:
            raise RuntimeError(f"{self.name}: backward called before forward.")

        grad_out_arr = np.asarray(grad_output, dtype=np.float32)

        if grad_out_arr.shape != self._mask.shape:
            raise ValueError(
                f"{self.name}: grad_output shape {grad_out_arr.shape} "
                f"does not match mask shape {self._mask.shape}"
            )

        # Elementwise multiply with mask
        grad_input = grad_out_arr * self._mask
        return grad_input

    # ------------------------------------------------------
    # Parameters / grads
    # ------------------------------------------------------
    def parameters(self):
        # ReLU has no learnable parameters
        return []

    def zero_grad(self):
        # Nothing to do; no parameters
        pass

    def __repr__(self):
        return f"{self.name}()"



# ===== scorch_sequential.py =====

# scorch/scorch_sequential.py
from __future__ import annotations

from typing import List

from scorch.scorch_module import ScorchModule


class ScorchSequential(ScorchModule):
    """
    A simple container that holds several ScorchModules and applies them in order.

    Forward pass:
        input → layer_0 → layer_1 → ... → layer_N → output

    Backward pass:
        final gradient → layer_N.backward → ... → layer_1.backward → layer_0.backward

    Parameters:
        Returns all parameters belonging to all child layers.

    zero_grad():
        Clears all stored gradients in every child layer.
    """

    def __init__(self, *layers: ScorchModule):
        """
        Example:
            model = ScorchSequential(
                ScorchLinear(4, 8),
                ScorchReLU(),
                ScorchLinear(8, 3)
            )
        """
        super().__init__()
        self.layers: List[ScorchModule] = list(layers)

    # ------------------------------------------------------
    # Forward pass
    # ------------------------------------------------------
    def forward(self, x):
        """
        Pass the input through each layer in order.

        x: whatever shape the first layer expects (often a 1-D np.ndarray)
        returns: output of the final layer
        """
        out = x
        for layer in self.layers:
            out = layer.forward(out)
        return out

    # ------------------------------------------------------
    # Backward pass
    # ------------------------------------------------------
    def backward(self, grad_output):
        """
        Backpropagate a gradient through all layers in reverse order.

        grad_output:
            The gradient of the loss with respect to the output of
            the last layer. (Often called "dL/dy_last")

        Returns:
            The gradient of the loss with respect to the original input.
            (Often called "dL/dx_first")
        """
        grad = grad_output

        # Traverse layers in reverse order (just like PyTorch)
        for layer in reversed(self.layers):
            grad = layer.backward(grad)

        return grad

    # ------------------------------------------------------
    # Parameter management
    # ------------------------------------------------------
    def parameters(self):
        """
        Collect parameters from every child layer into a single flat list.
        """
        params = []
        for layer in self.layers:
            params.extend(layer.parameters())
        return params

    def zero_grad(self):
        """
        Reset gradient buffers for all child layers.
        """
        for layer in self.layers:
            layer.zero_grad()

    # ------------------------------------------------------
    # Convenience
    # ------------------------------------------------------
    def append(self, layer: ScorchModule):
        """Add a layer to the end."""
        self.layers.append(layer)

    def __len__(self):
        return len(self.layers)

    def __getitem__(self, idx: int):
        return self.layers[idx]

    def __repr__(self):
        inner = ",\n  ".join(repr(layer) for layer in self.layers)
        return f"ScorchSequential(\n  {inner}\n)"



# ===== scorch_tensor.py =====

# scorch/scorch_tensor.py

from __future__ import annotations
from dataclasses import dataclass
from typing import Optional
import numpy as np
from image.bitmap import Bitmap
from image.rgba import RGBA

@dataclass
class ScorchTensor:
    """
    Core numeric tensor type for Scorch.

    Internal Rules:
    - ALWAYS normalized float32 in range [0, 1].
    - ALWAYS channel-first: (C, H, W).
    - C is 1 (grayscale) or 3 (RGB).
    """

    data: np.ndarray                  # MUST be float32 (C,H,W) normalized
    name: Optional[str] = None
    role: str = "generic"             # "image", "mask", etc.

    # ===============================================================
    # Bitmap → ScorchTensor
    # ===============================================================
    @classmethod
    def from_bitmap(
        cls,
        bmp: Bitmap,
        name: Optional[str] = None,
        role: str = "image",
        grayscale: bool = True,
    ) -> "ScorchTensor":
        """
        Convert a Bitmap to a ScorchTensor, ALWAYS normalized to [0,1].
        """
        if bmp.width <= 0 or bmp.height <= 0:
            return cls(np.zeros((0,), dtype=np.float32), name=name, role=role)

        # BGRA uint8 → (H, W, 4)
        bgra = bmp.export_opencv().astype(np.float32)

        # Extract channels
        b = bgra[:, :, 0]
        g = bgra[:, :, 1]
        r = bgra[:, :, 2]

        if grayscale:
            gray = RGBA.to_gray(r, g, b)     # H,W
            gray /= 255.0                                # normalize
            data = gray[np.newaxis, :, :]                # C=1,H,W
        else:
            r /= 255.0
            g /= 255.0
            b /= 255.0
            data = np.stack([r, g, b], axis=0)           # C=3,H,W

        return cls(data=data.astype(np.float32), name=name, role=role)
    
    @classmethod
    def from_bitmap_crop(
        cls,
        bmp: Bitmap,
        x: int,
        y: int,
        width: int,
        height: int,
        name: Optional[str] = None,
        role: str = "image",
        grayscale: bool = True,
    ) -> "ScorchTensor":
        """
        Crop region → Bitmap.crop → ScorchTensor.
        Always normalized.
        """
        cropped = bmp.crop(
            x=x,
            y=y,
            width=width,
            height=height)
        return cls.from_bitmap(cropped, name=name, role=role, grayscale=grayscale)
    

    # ===============================================================
    # Init hook
    # ===============================================================
    def __post_init__(self) -> None:
        # Force float32
        arr = np.asarray(self.data, dtype=np.float32)

        # Enforce normalization (final safety clamp)
        arr = np.clip(arr, 0.0, 1.0)

        # If 2D: promote to (1,H,W)
        if arr.ndim == 2:
            arr = arr[np.newaxis, :, :]  # grayscale assumption

        # Validate shape
        if arr.ndim != 3:
            raise ValueError(f"ScorchTensor must be 3D (C,H,W), got shape {arr.shape}")

        self.data = arr

    # ===============================================================
    # Introspection
    # ===============================================================
    @property
    def shape(self):
        return self.data.shape

    @property
    def ndim(self):
        return self.data.ndim
    

    def clone(self) -> "ScorchTensor":
        return ScorchTensor(self.data.copy(), name=self.name, role=self.role)

    def flatten(self) -> "ScorchTensor":
        c, h, w = self.data.shape
        return ScorchTensor(self.data.reshape(1, c * h * w), name=self.name, role=self.role)

    # ===============================================================
    # Framework Export
    # ===============================================================
    def to_numpy(self) -> np.ndarray:
        return self.data

    def to_torch(self):
        import torch
        return torch.from_numpy(self.data.copy())

    def to_tf(self):
        import tensorflow as tf
        arr = np.moveaxis(self.data, 0, -1)  # CHW → HWC
        return tf.convert_to_tensor(arr)

    # ===============================================================
    # ScorchTensor → Bitmap (for visualization)
    # ===============================================================
    def to_bitmap(self) -> Bitmap:
        """
        Convert this ScorchTensor back into a Bitmap.

        Assumes:
        - data ∈ [0,1]
        - shape = (C,H,W)
        - C = 1 or 3
        """
        arr = self.data

        if arr.size == 0:
            return Bitmap()

        c, h, w = arr.shape

        if c not in (1, 3):
            raise ValueError(f"Cannot convert tensor with C={c} to Bitmap")

        # Scale back to byte range
        img = np.clip(arr * 255.0, 0, 255).astype(np.uint8)

        # Allocate Bitmap
        bmp = Bitmap(w, h)

        if c == 1:
            # grayscale: broadcast to rgb
            gray = img[0, :, :]
            for x in range(w):
                col = bmp.rgba[x]
                for y in range(h):
                    v = int(gray[y, x])
                    px = col[y]
                    px.ri = v
                    px.gi = v
                    px.bi = v
                    px.ai = 255

        else:
            # RGB channels
            r = img[0, :, :]
            g = img[1, :, :]
            b = img[2, :, :]
            for x in range(w):
                col = bmp.rgba[x]
                for y in range(h):
                    px = col[y]
                    px.ri = int(r[y, x])
                    px.gi = int(g[y, x])
                    px.bi = int(b[y, x])
                    px.ai = 255

        return bmp
    
    def to_image(self):
        """
        Convert this ScorchTensor directly to a Pillow Image.

        Internally:
        - Uses to_bitmap() to rebuild a Bitmap
        - Then calls Bitmap.export_pillow() to get a Pillow Image
        """
        bmp = self.to_bitmap()
        return bmp.export_pillow()



# ===== tensor_load_helpers.py =====

# scorch/tensor_load_helpers.py

from __future__ import annotations
from typing import Iterator, Tuple
import numpy as np

from labels.pixel_bag import PixelBag
from scorch.scorch_tensor import ScorchTensor
from image.bitmap import Bitmap
from scorch.data_loader import AnnotationImagePair, DataLoader


# -------------------------------------------------------------------
# Core: mask from PixelBag (cropped to PixelBag.frame)
# -------------------------------------------------------------------
def mask_tensor_from_pixel_bag(
    bag: PixelBag,
    name: str | None = None,
) -> ScorchTensor:
    """
    Create a 1 x H x W mask tensor cropped to the PixelBag's frame.
    Pixels in the bag -> 1.0, everything else -> 0.0.
    """
    x0, y0, w, h = bag.frame

    if w == 0 or h == 0:
        return ScorchTensor(
            np.zeros((0,), dtype=np.float32),
            name=name,
            role="mask",
        )

    mask = np.zeros((1, h, w), dtype=np.float32)

    for x, y in bag:
        mx = x - x0  # local coords inside bbox
        my = y - y0
        if 0 <= mx < w and 0 <= my < h:
            mask[0, my, mx] = 1.0

    return ScorchTensor(mask, name=name, role="mask")


# -------------------------------------------------------------------
# Core: mask from PixelBag (fixed size, centered)
# -------------------------------------------------------------------
def mask_tensor_from_pixel_bag_fixed_size_centered(
    bag: PixelBag,
    fixed_width: int,
    fixed_height: int,
    name: str | None = None,
) -> ScorchTensor:
    """
    Create a 1 x fixed_height x fixed_width mask tensor.

    Steps:
      - Build the tight mask cropped to PixelBag.frame (same as
        mask_tensor_from_pixel_bag).
      - Place that mask centered inside a larger canvas of size
        (1, fixed_height, fixed_width).
      - The rest is zeros.

    Assumes that bag.frame width/height <= fixed_width / fixed_height.
    """
    x0, y0, w, h = bag.frame

    if w == 0 or h == 0:
        # Empty bag -> all zeros mask of requested size
        mask = np.zeros((1, fixed_height, fixed_width), dtype=np.float32)
        return ScorchTensor(mask, name=name, role="mask_fixed_size_centered")

    if w > fixed_width or h > fixed_height:
        raise ValueError(
            f"mask_tensor_from_pixel_bag_fixed_size_centered: "
            f"bag frame size ({w},{h}) exceeds fixed size "
            f"({fixed_width},{fixed_height})"
        )

    # Local tight mask
    tight = mask_tensor_from_pixel_bag(
        bag=bag,
        name=name,
    )
    tight_arr = tight.to_numpy()  # (1,h,w)

    mask = np.zeros((1, fixed_height, fixed_width), dtype=np.float32)

    # Center placement
    dx = fixed_width // 2 - w // 2
    dy = fixed_height // 2 - h // 2

    mask[:, dy:dy + h, dx:dx + w] = tight_arr

    return ScorchTensor(mask, name=name, role="mask_fixed_size_centered")


# -------------------------------------------------------------------
# Core: image + mask from PixelBag (one label, tight frame)
# -------------------------------------------------------------------
def image_and_mask_from_pixel_bag(
    bmp: Bitmap,
    bag: PixelBag,
    name: str | None = None,
    grayscale: bool = True,
) -> Tuple[ScorchTensor, ScorchTensor]:
    """
    Return (image_patch, mask_patch) for a single PixelBag.

    Both tensors are cropped to PixelBag.frame and share the same H, W.
    """
    x0, y0, w, h = bag.frame

    if w == 0 or h == 0:
        empty = ScorchTensor(
            np.zeros((0,), dtype=np.float32),
            name=name,
            role="empty",
        )
        return empty, empty

    # Defensive clamp to bitmap bounds
    x0 = max(0, min(x0, bmp.width  - w))
    y0 = max(0, min(y0, bmp.height - h))

    img_tensor = ScorchTensor.from_bitmap_crop(
        bmp=bmp,
        x=x0,
        y=y0,
        width=w,
        height=h,
        name=f"{name}_img" if name else None,
        role="image_patch",
        grayscale=grayscale,
    )

    mask_tensor = mask_tensor_from_pixel_bag(
        bag=bag,
        name=f"{name}_mask" if name else None,
    )

    # Simple shape sanity check
    _, h_m, w_m = mask_tensor.shape
    _, h_i, w_i = img_tensor.shape
    if (h_m, w_m) != (h_i, w_i):
        raise ValueError(
            f"Mask/image size mismatch: mask=({h_m},{w_m}), image=({h_i},{w_i})"
        )

    return img_tensor, mask_tensor


# -------------------------------------------------------------------
# Core: image + mask from PixelBag (fixed slab, centered)
# -------------------------------------------------------------------
def image_and_mask_from_pixel_bag_fixed_size_centered(
    bmp: Bitmap,
    bag: PixelBag,
    fixed_width: int,
    fixed_height: int,
    name: str | None = None,
    grayscale: bool = True,
) -> Tuple[ScorchTensor, ScorchTensor]:
    """
    Return (image_patch_fixed_size_centered, mask_patch_fixed_size_centered)
    for a single PixelBag.

    Steps:
      - Crop the bitmap to the PixelBag.frame (tight bounding box).
      - Verify that the frame fits inside the requested fixed size.
      - Create a new Bitmap(fixed_width, fixed_height).
      - Center-stamp the cropped bitmap into the fixed-size bitmap using:
            stamp_x = fixed_width  // 2 - w // 2
            stamp_y = fixed_height // 2 - h // 2
      - Build a matching fixed-size mask where the tight mask is
        placed at the same centered location.

    Result:
      - image_patch_fixed_size_centered: ScorchTensor, (C, fixed_height, fixed_width)
      - mask_patch_fixed_size_centered:  ScorchTensor, (1, fixed_height, fixed_width)
    """
    x0, y0, w, h = bag.frame

    if w == 0 or h == 0:
        # Empty bag -> just return all-zero tensors of the fixed size
        img_arr = np.zeros((1, fixed_height, fixed_width), dtype=np.float32)
        mask_arr = np.zeros((1, fixed_height, fixed_width), dtype=np.float32)
        img_tensor = ScorchTensor(
            img_arr,
            name=f"{name}_img_fixed_size_centered" if name else None,
            role="image_patch_fixed_size_centered",
        )
        mask_tensor = ScorchTensor(
            mask_arr,
            name=f"{name}_mask_fixed_size_centered" if name else None,
            role="mask_fixed_size_centered",
        )
        return img_tensor, mask_tensor

    if w > fixed_width or h > fixed_height:
        raise ValueError(
            f"image_and_mask_from_pixel_bag_fixed_size_centered: "
            f"bag frame size ({w},{h}) exceeds fixed size "
            f"({fixed_width},{fixed_height})"
        )

    # Defensive clamp to bitmap bounds, same as non-fixed helper
    x0 = max(0, min(x0, bmp.width  - w))
    y0 = max(0, min(y0, bmp.height - h))

    # 1) Tight crop around the bag
    cropped = bmp.crop(
        x=x0,
        y=y0,
        width=w,
        height=h,
    )

    # 2) New fixed-size bitmap and center stamp
    padded = Bitmap(fixed_width, fixed_height)

    stamp_x = fixed_width // 2 - cropped.width // 2
    stamp_y = fixed_height // 2 - cropped.height // 2

    padded.stamp(cropped, stamp_x, stamp_y)

    # 3) Convert to ScorchTensor
    img_tensor = ScorchTensor.from_bitmap(
        bmp=padded,
        name=f"{name}_img_fixed_size_centered" if name else None,
        role="image_patch_fixed_size_centered",
        grayscale=grayscale,
    )

    # 4) Matching fixed-size mask
    mask_tensor = mask_tensor_from_pixel_bag_fixed_size_centered(
        bag=bag,
        fixed_width=fixed_width,
        fixed_height=fixed_height,
        name=f"{name}_mask_fixed_size_centered" if name else None,
    )

    # Final sanity check: shapes must match
    _, h_m, w_m = mask_tensor.shape
    _, h_i, w_i = img_tensor.shape
    if (h_m, w_m) != (h_i, w_i):
        raise ValueError(
            f"Fixed-size mask/image size mismatch: "
            f"mask=({h_m},{w_m}), image=({h_i},{w_i})"
        )

    return img_tensor, mask_tensor


# -------------------------------------------------------------------
# High-level: iterate (image_patch, mask_patch, label_name) over a pair
# -------------------------------------------------------------------
def iter_label_patches_from_pair(
    pair: AnnotationImagePair,
    grayscale: bool = True,
):
    """
    Yield (image_patch, mask_patch, label_name) for every label
    in a single AnnotationImagePair.

    Uses the tight PixelBag.frame (no padding).
    """
    bmp = Bitmap.with_image(pair.image_path)
    doc = pair.document

    # Adjust attribute names if your document layout differs
    for label in doc.data.labels:
        bag = label.pixel_bag
        img_patch, mask_patch = image_and_mask_from_pixel_bag(
            bmp=bmp,
            bag=bag,
            name=label.name,
            grayscale=grayscale,
        )
        yield img_patch, mask_patch, label.name


# -------------------------------------------------------------------
# High-level: iterate fixed-size centered patches over a pair
# -------------------------------------------------------------------
def iter_label_patches_from_pair_fixed_size_centered(
    pair: AnnotationImagePair,
    fixed_width: int,
    fixed_height: int,
    grayscale: bool = True,
):
    """
    Yield (image_patch_fixed_size_centered, mask_patch_fixed_size_centered,
    label_name) for every label in a single AnnotationImagePair.

    Each patch is exactly (fixed_height, fixed_width) with the original
    PixelBag.frame content centered in the slab.
    """
    bmp = Bitmap.with_image(pair.image_path)
    doc = pair.document

    for label in doc.data.labels:
        bag = label.pixel_bag
        img_patch, mask_patch = image_and_mask_from_pixel_bag_fixed_size_centered(
            bmp=bmp,
            bag=bag,
            fixed_width=fixed_width,
            fixed_height=fixed_height,
            name=label.name,
            grayscale=grayscale,
        )
        yield img_patch, mask_patch, label.name



# ========== ROOT: /Users/naraptis/Desktop/Combustion/labels ==========

# ===== data_label.py =====

# data_label.py
from __future__ import annotations

from typing import Any, Dict
from labels.pixel_bag import PixelBag

class DataLabel:
    """
    Associates a string name with a PixelBag.

    JSON format:

        {
            "name": "some_label_name",
            "pixels": [ { "y": ..., "x_start": ..., "x_end": ... }, ... ]
        }
    """

    def __init__(self, name: str, pixel_bag: PixelBag | None = None) -> None:
        self.name: str = name
        self.pixel_bag: PixelBag = pixel_bag if pixel_bag is not None else PixelBag()


    # --------------------------------------------------
    # PixelBag passthrough wrappers
    # --------------------------------------------------
    def clear(self) -> None:
        self.pixel_bag.clear()

    def add(self, x: int, y: int) -> None:
        self.pixel_bag.add(x, y)

    def remove(self, x: int, y: int) -> None:
        self.pixel_bag.remove(x, y)

    def contains(self, x: int, y: int) -> bool:
        return self.pixel_bag.contains(x, y)

    # --------------------------------------------------
    # JSON serialization
    # --------------------------------------------------
    def to_json(self) -> Dict[str, Any]:
        return {
            "name": self.name,
            "pixels": self.pixel_bag.to_json(),
        }

    @staticmethod
    def from_json(data: Dict[str, Any]) -> "DataLabel":
        name = data.get("name", "")
        pixels_json = data.get("pixels", [])
        bag = PixelBag.from_json(pixels_json)
        return DataLabel(name=name, pixel_bag=bag)

    # --------------------------------------------------
    # Utility / debug printing (summary style)
    # --------------------------------------------------
    def __repr__(self) -> str:
        """
        One-line compact summary, e.g.:

            DataLabel(name="lymph",
                      bag=PixelBag(count=100, median=(15,35), size=(10,10)))

        If bag is empty:

            DataLabel(name="lymph", bag=PixelBag(count=0))
        """
        return f'DataLabel(name="{self.name}", bag={self.pixel_bag})'



# ===== data_label_collection.py =====

# data_label_collection.py
from __future__ import annotations

from typing import Any, Dict, List, Iterable

from labels.data_label import DataLabel

class DataLabelCollection:
    """
    A simple list of DataLabel objects.

    Multiple labels may share the same name. Each DataLabel typically
    represents one instance (one PixelBag region).
    """

    def __init__(self, labels: List[DataLabel] | None = None) -> None:
        self.labels: List[DataLabel] = labels or []

    # --------------------------------------------------
    # Basic operations
    # --------------------------------------------------
    def add_label(self, label: DataLabel) -> None:
        """
        Append a label. Multiple labels may have the same name.
        """
        self.labels.append(label)

    def remove_label(self, label: DataLabel) -> None:
        """
        Remove this exact label object from the collection, if present.
        Does nothing if the label is not in the collection.
        """
        try:
            self.labels.remove(label)
        except ValueError:
            # label not in list; ignore
            pass

    def get_labels_by_name(self, name: str) -> List[DataLabel]:
        """
        Return all labels whose name matches the given name.
        May return an empty list.
        """
        return [lbl for lbl in self.labels if lbl.name == name]

    def first_label(self, name: str) -> DataLabel | None:
        """
        Convenience: return the first label with this name, or None.
        """
        for lbl in self.labels:
            if lbl.name == name:
                return lbl
        return None

    # --------------------------------------------------
    # JSON serialization (array only)
    # --------------------------------------------------
    def to_json(self) -> List[Dict[str, Any]]:
        """
        Return a JSON-compatible list of label dicts, no wrapper.
        """
        return [label.to_json() for label in self.labels]

    @staticmethod
    def from_json(data: List[Dict[str, Any]]) -> "DataLabelCollection":
        """
        Parse a list of label objects.
        """
        labels = [DataLabel.from_json(item) for item in data]
        return DataLabelCollection(labels=labels)

    # --------------------------------------------------
    # Helpers
    # --------------------------------------------------
    def __len__(self):
        return len(self.labels)

    def __iter__(self) -> Iterable[DataLabel]:
        return iter(self.labels)

    def _sorted_labels(self) -> List[DataLabel]:
        """
        Return labels sorted by:
          1) name (alphabetical)
          2) median.y  (ascending)
          3) median.x  (ascending)

        Uses PixelBag.summary().median so we don't have to look at stripes.
        Empty bags get a large sentinel so they sort last within a name.
        """
        if not self.labels:
            return []

        def sort_key(label: DataLabel):
            name = label.name
            summary = label.pixel_bag.summary()
            median = summary.get("median")

            if median is None:
                # Empty bag: push to the end for that name
                my = 10**9
                mx = 10**9
            else:
                mx, my = median  # median is (x, y)

            return (name, my, mx)

        return sorted(self.labels, key=sort_key)

    def __repr__(self) -> str:
        """
        Compact summary of the collection plus one-line summary per label, e.g.:

            DataLabelCollection(count=2):
                DataLabel(name="basal", bag=PixelBag(...))
                DataLabel(name="lymph", bag=PixelBag(...))
        """
        count = len(self.labels)
        if count == 0:
            return "DataLabelCollection(count=0)"

        lines: List[str] = [f"DataLabelCollection(count={count}):"]
        for label in self._sorted_labels():
            lines.append(f"    {label!r}")
        return "\n".join(lines)
    


# ===== image_annotation_document.py =====

# image_annotation_document.py
from __future__ import annotations

from typing import Any, Dict, List
from labels.data_label_collection import DataLabelCollection

class ImageAnnotationDocument:
    """
    Top-level container for annotations for a single image.

    Fields:
      - name:             Logical name for this document (often the JSON filename).
      - width:            Image width in pixels.
      - height:           Image height in pixels.
      - data:             DataLabelCollection holding all individual labels.
      - data_label_names: Derived list of label names present in `data`.
    """

    def __init__(
        self,
        name: str,
        width: int,
        height: int,
        data: DataLabelCollection | None = None,
    ) -> None:
        self.name: str = name
        self.width: int = int(width)
        self.height: int = int(height)
        # data is conceptually non-optional; default to empty collection if None
        self.data: DataLabelCollection = data if data is not None else DataLabelCollection()

    # --------------------------------------------------
    # Derived properties
    # --------------------------------------------------
    @property
    def data_label_names(self) -> List[str]:
        """
        Return a sorted list of unique label names present in this document's data.
        """
        names = {label.name for label in self.data}
        return sorted(names)

    # --------------------------------------------------
    # JSON serialization
    # --------------------------------------------------
    def to_json(self) -> Dict[str, Any]:
        """
        Return a JSON-compatible dict representing this document.

        Layout:

        {
          "name":             str,
          "width":            int,
          "height":           int,
          "data_label_names": [str, ...],
          "labels":           [ ... DataLabel.to_json() ... ]
        }

        Note: data_label_names is derived from `data` at serialization time.
        """
        return {
            "name": self.name,
            "width": self.width,
            "height": self.height,
            "data_label_names": self.data_label_names,
            "labels": self.data.to_json(),
        }

    @staticmethod
    def from_json(data: Dict[str, Any]) -> "ImageAnnotationDocument":
        """
        Parse an ImageAnnotationDocument from a JSON-compatible dict.

        data_label_names from JSON are currently ignored as a source of truth,
        since they can always be recomputed from the label data. They are
        still read (if present) to allow future validation if desired.
        """
        name = data.get("name", "")
        width = int(data.get("width", 0))
        height = int(data.get("height", 0))

        labels_raw = data.get("labels", []) or []
        dlc = DataLabelCollection.from_json(labels_raw)

        # data_label_names = data.get("data_label_names", [])  # optional; not required

        return ImageAnnotationDocument(
            name=name,
            width=width,
            height=height,
            data=dlc,
        )

    # --------------------------------------------------
    # Debug / repr
    # --------------------------------------------------
    def __repr__(self) -> str:
        """
        Compact summary plus an indented listing of labels via
        DataLabelCollection.__repr__.

        Example:

            ImageAnnotationDocument(name="sample_01",
                                    size=(256, 256),
                                    data_label_names=['Alphacyte', 'Lymphocyte'],
                                    label_count=3):
                DataLabel(name="Alphacyte", bag=PixelBag(...))
                DataLabel(name="Lymphocyte", bag=PixelBag(...))
                ...
        """
        label_count = len(self.data)
        header = (
            f'ImageAnnotationDocument('
            f'name="{self.name}", '
            f'size=({self.width}, {self.height}), '
            f'data_label_names={self.data_label_names}, '
            f'label_count={label_count})'
        )

        if label_count == 0:
            return header

        # Indent the DataLabelCollection repr by 4 spaces
        dlc_lines = repr(self.data).splitlines()
        indented_dlc = "\n".join("    " + line for line in dlc_lines)

        return header + ":\n" + indented_dlc



# ===== pixel_bag.py =====

# pixel_bag.py
from __future__ import annotations
from typing import Any, List
from labels.pixel_bag_run_length import PixelBagRunLength
from labels.pixel_bag_run_length_stripe import PixelBagRunLengthStripe

class PixelBag:
    """
    Stores a set of (x, y) integer pixel positions.
    Does not store duplicates (internally a set),
    but add/remove semantics allow double-add and remove-nonexistent
    without raising exceptions.
    """

    def __init__(self) -> None:
        self._set = set()   # stores (x, y)

    # --------------------------------------------------
    # Basic operations
    # --------------------------------------------------
    def clear(self) -> None:
        self._set.clear()

    def add(self, x: int, y: int) -> None:
        """Add a pixel. Adding an existing pixel is allowed and ignored."""
        self._set.add((int(x), int(y)))

    def remove(self, x: int, y: int) -> None:
        """Remove a pixel. Removing a missing pixel is allowed and ignored."""
        self._set.discard((int(x), int(y)))  # discard() never raises

    def contains(self, x: int, y: int) -> bool:
        """Check if a pixel exists in the bag."""
        return (int(x), int(y)) in self._set

    # --------------------------------------------------
    # Bounding box helpers
    # --------------------------------------------------
    @property
    def xmin(self):
        if not self._set:
            return None
        return min(px for (px, _) in self._set)

    @property
    def xmax(self):
        if not self._set:
            return None
        return max(px for (px, _) in self._set)

    @property
    def ymin(self):
        if not self._set:
            return None
        return min(py for (_, py) in self._set)

    @property
    def ymax(self):
        if not self._set:
            return None
        return max(py for (_, py) in self._set)
    
    @property
    def frame(self):
        if not self._set:
            return (0, 0, 0, 0)

        xr = self.xrange()
        yr = self.yrange()
        
        width  = xr.stop - xr.start
        height = yr.stop - yr.start

        return (xr.start, yr.start, width, height)

    # --------------------------------------------------
    # Ranges for easy looping
    # --------------------------------------------------
    def xrange(self):
        """
        Return range(xmin, xmax+1).
        If empty, return range(0).
        """
        if not self._set:
            return range(0)
        return range(self.xmin, self.xmax + 1)

    def yrange(self):
        """
        Return range(ymin, ymax+1).
        If empty, return range(0).
        """
        if not self._set:
            return range(0)
        return range(self.ymin, self.ymax + 1)

    # --------------------------------------------------
    # Utility
    # --------------------------------------------------
    def __len__(self):
        return len(self._set)

    def __iter__(self):
        """Iterate over (x, y) pairs."""
        return iter(self._set)

    def summary(self) -> dict:
        """
        Return summary statistics:

            {
            "count": int,
            "median": (x_med, y_med) | None,
            "size": (width, height)
            }

        width  = xmax - xmin + 1
        height = ymax - ymin + 1
        """
        count = len(self._set)
        if count == 0:
            return {
                "count": 0,
                "median": None,
                "size": (0, 0),
            }

        xs = [x for (x, _) in self._set]
        ys = [y for (_, y) in self._set]

        xs_sorted = sorted(xs)
        ys_sorted = sorted(ys)
        mid = count // 2

        median_x = xs_sorted[mid]
        median_y = ys_sorted[mid]

        xmin = self.xmin
        xmax = self.xmax
        ymin = self.ymin
        ymax = self.ymax

        width = (xmax - xmin + 1) if xmin is not None and xmax is not None else 0
        height = (ymax - ymin + 1) if ymin is not None and ymax is not None else 0

        return {
            "count": count,
            "median": (median_x, median_y),
            "size": (width, height),
        }


    def __repr__(self):
        """
        Compact one-line summary:

            PixelBag(count=67, median=(46,77), size=(80, 111))
        """
        info = self.summary()
        count = info["count"]

        if count == 0:
            return "PixelBag(count=0)"

        mx, my = info["median"]
        w, h = info["size"]

        return (
            f"PixelBag(count={count}, "
            f"median=({mx}, {my}), "
            f"size=({w}, {h}))"
        )


    # --------------------------------------------------
    # Run-length conversion
    # --------------------------------------------------
    def to_run_length(self) -> "PixelBagRunLength":
        result = PixelBagRunLength()

        _xmin = self.xmin
        _xmax = self.xmax
        if _xmin is None or _xmax is None:
            return result

        for y in self.yrange():
            x = _xmin
            while x <= _xmax:
                if self.contains(x, y):
                    x_start = x
                    x_end = x
                    x += 1
                    while x <= _xmax and self.contains(x, y):
                        x_end = x
                        x += 1
                    result.add_stripe(PixelBagRunLengthStripe(y, x_start, x_end))
                else:
                    x += 1
        return result

    # --------------------------------------------------
    # JSON serialization
    # --------------------------------------------------
    def to_json(self) -> List[Any]:
        """
        Serialize this PixelBag to a JSON-compatible list of run-length
        stripe objects:
            [ { "y": ..., "x_start": ..., "x_end": ... }, ... ]
        """
        rle = self.to_run_length()
        return rle.to_json()

    @staticmethod
    def from_json(data: List[Any]) -> "PixelBag":
        """
        Deserialize a PixelBag from a JSON-compatible list produced
        by PixelBag.to_json().

        Reconstructs all (x, y) pixels from the run-length stripes.
        """
        rle = PixelBagRunLength.from_json(data)
        bag = PixelBag()
        for stripe in rle.stripes:
            y = stripe.y
            for x in range(stripe.x_start, stripe.x_end + 1):
                bag.add(x, y)
        return bag



# ===== pixel_bag_run_length.py =====

# pixel_bag_run_length.py
from __future__ import annotations
from typing import Any, Iterable, List

from labels.pixel_bag_run_length_stripe import PixelBagRunLengthStripe

class PixelBagRunLength:
    """
    Run-length representation of a PixelBag:
    a list of horizontal stripes.
    """

    def __init__(self, stripes: List[PixelBagRunLengthStripe] | None = None) -> None:
        self.stripes: List[PixelBagRunLengthStripe] = stripes or []

    # --------------------------------------------------
    # JSON serialization
    # --------------------------------------------------
    def to_json(self) -> List[Any]:
        return [stripe.to_json() for stripe in self.stripes]

    @staticmethod
    def from_json(data: List[Any]) -> "PixelBagRunLength":
        stripes = [PixelBagRunLengthStripe.from_json(item) for item in data]
        return PixelBagRunLength(stripes=stripes)

    # --------------------------------------------------
    # Utility
    # --------------------------------------------------
    def add_stripe(self, stripe: PixelBagRunLengthStripe) -> None:
        self.stripes.append(stripe)

    def __len__(self) -> int:
        return len(self.stripes)

    def __iter__(self):
        return iter(self.stripes)

    # -------- central sorting helper --------
    @staticmethod
    def sorted_stripes(
        stripes: Iterable[PixelBagRunLengthStripe],
    ) -> List[PixelBagRunLengthStripe]:
        return sorted(stripes, key=lambda s: (s.y, s.x_start))

    def sorted(self) -> List[PixelBagRunLengthStripe]:
        return PixelBagRunLength.sorted_stripes(self.stripes)

    # --------------------------------------------------
    # One-line summary repr
    # --------------------------------------------------
    def __repr__(self) -> str:
        """
        Compact one-line summary:
        PixelBagRunLength(count=3, stripes=[Stripe(y=10,3→5), Stripe(y=11,7→7)])
        """
        if not self.stripes:
            return "PixelBagRunLength(count=0, stripes=[])"

        parts = []
        for s in self.sorted():
            parts.append(f"Stripe(y={s.y},{s.x_start}→{s.x_end})")

        stripes_str = ", ".join(parts)
        return f"PixelBagRunLength(count={len(self.stripes)}, stripes=[{stripes_str}])"



# ===== pixel_bag_run_length_stripe.py =====

# pixel_bag_run_length_stripe.py
from __future__ import annotations
from dataclasses import dataclass
from typing import Any, Dict

@dataclass
class PixelBagRunLengthStripe:
    """
    Represents a horizontal run of pixels at a fixed y:
        x in [x_start, x_end] inclusive.
    """
    y: int
    x_start: int
    x_end: int

    def to_json(self) -> Dict[str, Any]:
        """
        Serialize this stripe to a JSON-compatible dict.
        """
        return {
            "y": int(self.y),
            "x_start": int(self.x_start),
            "x_end": int(self.x_end),
        }

    @staticmethod
    def from_json(data: Dict[str, Any]) -> "PixelBagRunLengthStripe":
        """
        Deserialize a stripe from a JSON-compatible dict.
        Expects keys: "y", "x_start", "x_end".
        """
        return PixelBagRunLengthStripe(
            y=int(data["y"]),
            x_start=int(data["x_start"]),
            x_end=int(data["x_end"]),
        )

    # --------------------------------------------------
    # Debug / printing helpers
    # --------------------------------------------------
    def repr_str(self, indent: int = 0) -> str:
        """
        Return a single-line string representation, with leading tabs
        based on indent depth.
        """
        prefix = "\t" * max(indent, 0)
        return f"{prefix}(y={self.y}, x_start={self.x_start}, x_end={self.x_end})"

    def __repr__(self) -> str:
        """
        Default repr with no indentation.
        """
        return self.repr_str(indent=0)



